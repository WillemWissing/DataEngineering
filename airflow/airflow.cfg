[core]
# Airflow home
AIRFLOW_HOME = /opt/airflow
dags_folder = /opt/airflow/dags
# Executor
executor = CeleryExecutor

# SQL Alchemy connnection
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow?connect_timeout=10
# Fernet key to encrypt credentials
fernet_key = dWiDfewcrBlPNJ0-MmrQDGC5eCVeV0q6YXBikeyhBMw=

# Airflow web server
dags_are_paused_at_creation = true
load_examples = false
api_auth_backend = airflow.api.auth.backend.basic_auth
scheduler_health_check = true
http_timeout = 120
enable_xcom_pickling = True

[logging]
# Logging
remote_logging = False
base_log_folder = /opt/airflow/logs
logging_level = DEBUG

[webserver]
# Webserver configuration
web_server_host = 0.0.0.0
web_server_port = 8080

[celery]
# Celery configuration
broker_url = redis://:@airflow-redis:6379/0
result_backend = db+postgresql://airflow:airflow@airflow-postgres/airflow
worker_concurrency = 4
worker_prefetch_multiplier = 1  
task_acks_late = True  
worker_max_tasks_per_child = 100  
worker_max_memory_per_child = 300000  
# task_serializer = 'json'
# accept_content = ['json','pickle']
# result_serializer = 'json'

[security]
# Security settings
fernet_key = dWiDfewcrBlPNJ0-MmrQDGC5eCVeV0q6YXBikeyhBMw=

[api]
# API authentication
auth_backend = airflow.api.auth.backend.basic_auth

[database]
# Retry logic for database connections
sql_alchemy_conn_retry_max = 10 
sql_alchemy_conn_retry_delay = 10  

sql_alchemy_pool_size = 20
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 1800 
sql_alchemy_conn_retry_backoff = 2  

